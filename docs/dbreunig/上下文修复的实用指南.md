标题：如何修复你的上下文

来源地址：https://www.dbreunig.com/2025/06/26/how-to-fix-your-context.html

发布时间：2025-06-26T08:23:00-07:00

![Image 1](https://www.dbreunig.com/img/overload.jpg)

### 缓解和避免上下文失效

作为我们之前文章"[长上下文如何失效](https://www.dbreunig.com/2025/06/22/how-contexts-fail-and-how-to-fix-them.html)"的后续，让我们看看如何缓解或完全避免这些失效。

在开始之前，让我们简要回顾一下长上下文可能失效的几种方式：

* **上下文污染：** 当幻觉或其他错误进入上下文，并被反复引用时。
* **上下文分散：** 当上下文变得太长，导致模型过度关注上下文，忽略了在训练期间学到的知识时。
* **上下文混乱：** 当上下文中的多余信息被模型用来生成低质量响应时。
* **上下文冲突：** 当你在上下文中积累的新信息和工具与上下文中的其他信息发生冲突时。

这一切都与信息管理有关。上下文中的一切都会影响响应。我们又回到了古老的编程格言："[垃圾进，垃圾出](https://en.wikipedia.org/wiki/Garbage_in,_garbage_out)"。幸运的是，有很多选项来处理上述问题。

上下文管理策略
--------------

* [**RAG：** 有选择地添加相关信息以帮助LLM生成更好的响应](https://www.dbreunig.com/2025/06/26/how-to-fix-your-context.html#rag)
* [**工具配置：** 仅选择相关的工具定义添加到你的上下文中](https://www.dbreunig.com/2025/06/26/how-to-fix-your-context.html#tool-loadout)
* [**上下文隔离：** 在各自专用的线程中隔离上下文](https://www.dbreunig.com/2025/06/26/how-to-fix-your-context.html#context-quarantine)
* [**上下文修剪：** 从上下文中移除不相关或不需要的信息](https://www.dbreunig.com/2025/06/26/how-to-fix-your-context.html#context-pruning)
* [**上下文总结：** 将累积的上下文压缩成精简摘要](https://www.dbreunig.com/2025/06/26/how-to-fix-your-context.html#context-summarization)
* [**上下文卸载：** 将信息存储在LLM上下文之外，通常通过存储和管理数据的工具](https://www.dbreunig.com/2025/06/26/how-to-fix-your-context.html#context-offloading)

* * *

![Image 2: A stack of papers.](https://www.dbreunig.com/img/rag.png)

### RAG

*检索增强生成（RAG）是有选择地添加相关信息以帮助LLM生成更好响应的行为。*

关于RAG已经写了太多，我们今天不会深入讨论，只是说：它非常活跃。

每当一个模型提高上下文窗口的标准时，就会诞生一个新的"RAG已死"辩论。最后一个重大事件是Llama 4 Scout推出了*1000万标记窗口*。在这个规模下，真的很诱人去想，"算了，全部扔进去"，然后收工。

但是，正如我们上次所说：如果你把上下文当作杂物抽屉，杂物就会[影响你的响应](https://www.dbreunig.com/2025/06/22/how-contexts-fail-and-how-to-fix-them.html#context-confusion)。如果你想了解更多，这里有一个[看起来很棒的新课程](https://maven.com/p/569540/i-don-t-use-rag-i-just-retrieve-documents)。

![Image 3: A selection of guns from the videogame Destiny](https://www.dbreunig.com/img/tool_loadout.png)

### 工具配置

*工具配置是仅选择相关工具定义添加到你的上下文中的行为。*

"配置"这个术语是游戏术语，指你在关卡、匹配或回合之前选择的能力、武器和装备的特定组合。通常，你的配置是根据上下文量身定制的——角色、关卡、团队其他成员的构成，以及你自己的技能组合。

在这里，我们借用这个术语来描述为给定任务选择最相关的工具。

也许选择工具最简单的方法是将RAG应用于你的工具描述。这正是Tiantian Gan和Qiyao Sun所做的，他们在论文"[RAG MCP](https://arxiv.org/abs/2505.03275)"中详细介绍了这一点。通过将工具描述存储在向量数据库中，他们能够根据输入提示词选择最相关的工具。

在提示DeepSeek-v3时，团队发现当你有超过30个工具时，选择正确的工具变得至关重要。超过30个，工具的描述开始重叠，造成混乱。超过*100个工具*，模型几乎肯定会在他们的测试中失败。使用RAG技术选择少于30个工具产生了显著更短的提示词，并导致工具选择准确性提高了多达3倍。

对于较小的模型，问题在我们达到30个工具之前就开始了。我们在上一篇文章中提到的一篇论文"[少即是多](https://arxiv.org/abs/2411.15399)"表明，当给出46个工具时，Llama 3.1 8b在基准测试中失败，但只给出19个工具时成功。问题是上下文混乱，*不是*上下文窗口限制。

为了解决这个问题，"少即是多"背后的团队开发了一种使用LLM驱动的工具推荐器动态选择工具的方法。LLM被提示推理"它'认为'需要回答用户查询的工具数量和类型"。然后对此输出进行语义搜索（再次是工具RAG）以确定最终配置。他们用[伯克利函数调用排行榜](https://gorilla.cs.berkeley.edu/leaderboard.html)测试了这种方法，发现Llama 3.1 8b的性能提高了44%。

"少即是多"论文指出了较小上下文的另外两个好处：降低功耗和提高速度，这些是在边缘操作（意思是在你的手机或PC上运行LLM，而不是在专用服务器上）时的关键指标。即使他们的动态工具选择方法*未能*改善模型的结果，功耗节省和速度提升也是值得的，分别产生了18%和77%的节省。

幸运的是，大多数智能体的表面积较小，只需要几个手工策划的工具。但如果功能的广度或集成的数量需要扩展，总是要考虑你的配置。

![Image 4: A belljar](https://www.dbreunig.com/img/context_quarantine.png)

### 上下文隔离

*上下文隔离是在各自专用的线程中隔离上下文的行为，每个线程由一个或多个LLM单独使用。*

当我们的上下文不太长且不包含不相关内容时，我们会看到更好的结果。实现这一点的一种方法是将我们的任务分解为较小的、隔离的工作——每个都有自己的上下文。

有[许多](https://arxiv.org/abs/2402.14207)[这种策略的例子](https://www.microsoft.com/en-us/research/articles/magentic-one-a-generalist-multi-agent-system-for-solving-complex-tasks/)，但这种策略的一个易于理解的描述是Anthropic的[详细介绍其多智能体研究系统的博客文章](https://www.anthropic.com/engineering/built-multi-agent-research-system)。他们写道：

> 搜索的本质是压缩：从庞大的语料库中提炼见解。子智能体通过在各自的上下文窗口中并行操作来促进压缩，在将最重要的标记凝聚给主要研究智能体之前，同时探索问题的不同方面。每个子智能体还提供关注点分离——不同的工具、提示词和探索轨迹——这减少了路径依赖性并支持彻底、独立的调查。

研究适合这种设计模式。当给出一个问题时，可以识别几个子问题或探索领域，并使用多个智能体分别提示。这不仅加快了信息收集和提炼（如果有可用的计算资源），而且防止每个上下文积累太多信息或与给定提示词不相关的信息，从而提供更高质量的结果：

> 我们的内部评估显示，多智能体研究系统在涉及同时追求多个独立方向的广度优先查询方面特别出色。我们发现，以Claude Opus 4为主智能体、Claude Sonnet 4子智能体的多智能体系统在我们的内部研究评估中比单智能体Claude Opus 4系统表现好90.2%。例如，当被要求识别信息技术标普500公司的所有董事会成员时，多智能体系统通过将此分解为子智能体的任务找到了正确答案，而单智能体系统在缓慢的顺序搜索中未能找到答案。

这种方法也有助于工具配置，因为智能体设计者可以创建几个智能体原型，每个都有自己专用的配置和如何使用每个工具的指令。

那么，智能体构建者的挑战是找到将隔离任务分离到单独线程的机会。需要在多个智能体之间共享上下文的问题不太适合这种策略。

如果你的智能体领域在任何程度上都适合并行化，一定要[阅读整个Anthropic的文章](https://www.anthropic.com/engineering/built-multi-agent-research-system)。它非常出色。

![Image 5: Pruning shears](https://www.dbreunig.com/img/context_pruning.png)

### 上下文修剪

*上下文修剪是从上下文中移除不相关或不需要信息的行为。*

智能体在启动工具和组装文档时积累上下文。有时，值得暂停评估已组装的内容并移除垃圾。这可能是你分配给主LLM的任务，或者你可以设计一个单独的LLM驱动工具来审查和编辑上下文。或者你可以选择更针对修剪任务的东西。

上下文修剪有（相对）悠久的历史，因为在ChatGPT之前的自然语言处理（NLP）领域，上下文长度是一个更成问题的瓶颈。基于这一历史的当前修剪方法是[Provence](https://arxiv.org/abs/2501.16214)，"一个用于问答的高效且鲁棒的上下文修剪器"。

Provence快速、准确、易于使用且相对较小——只有1.75 GB。你可以用几行代码调用它，像这样：

```python
from transformers import AutoModel

provence = AutoModel.from_pretrained("naver/provence-reranker-debertav3-v1", trust_remote_code=True)

# 读取阿拉米达，加州维基百科条目的markdown版本
with open('alameda_wiki.md', 'r', encoding='utf-8') as f:
    alameda_wiki = f.read()

# 根据问题修剪文章
question = '离开阿拉米达我有什么选择？'
provence_output = provence.process(question, alameda_wiki)
```

Provence编辑了文章，删减了95%的内容，只留下[这个相关的子集](https://gist.github.com/dbreunig/b3bdd9eb34bc264574954b2b954ebe83)。它做得很好。

可以使用Provence或类似功能来删减文档或整个上下文。此外，这种模式有力地支持维护上下文的*结构化*[1](https://www.dbreunig.com/2025/06/26/how-to-fix-your-context.html#fn:structure)版本，使用字典或其他形式，在每次LLM调用之前从中组装编译的字符串。这种结构在修剪时会派上用场，允许你确保保留主要指令和目标，而文档或历史部分可以被修剪或总结。

![Image 6: A duck press](https://www.dbreunig.com/img/context_summarization.png)

### 上下文总结

*上下文总结是将累积的上下文压缩成精简摘要的行为。*

上下文总结最初出现是为了处理较小的上下文窗口。当你的聊天会话接近超出最大上下文长度时，会生成摘要并开始新的线程。聊天机器人用户在ChatGPT或Claude中手动执行此操作，要求机器人生成简短的回顾，然后将其粘贴到新会话中。

然而，随着上下文窗口的增加，智能体构建者发现总结除了保持在总上下文限制内之外还有好处。随着上下文的增长，它变得分散注意力，导致模型较少依赖在训练期间学到的知识。我们称之为[上下文分散](https://www.dbreunig.com/2025/06/22/how-contexts-fail-and-how-to-fix-them.html#context-distraction)。玩宝可梦的Gemini智能体背后的团队发现，超过100,000个标记的任何内容都会触发这种行为：

> 虽然Gemini 2.5 Pro支持100万+标记上下文，但在智能体中有效利用它是一个新的研究前沿。在这种智能体设置中，观察到当上下文显著超过10万标记时，智能体表现出倾向于重复其庞大历史中的动作，而不是综合新颖的计划。这种现象虽然是轶事性的，但突出了检索用长上下文和多步骤生成推理用长上下文之间的重要区别。

总结你的上下文很容易做到，但对于任何给定的智能体来说很难完善。知道应该保留什么信息，并将其详细说明给LLM驱动的压缩步骤，对智能体构建者来说至关重要。值得将此功能分解为自己的LLM驱动阶段或应用程序，这允许你收集评估数据，可以直接告知和优化此任务。

![Image 7: A banker's box](https://www.dbreunig.com/img/context_offload.png)

### 上下文卸载

*上下文卸载是将信息存储在LLM上下文之外的行为，通常通过存储和管理数据的工具。*

这可能是我最喜欢的策略，如果只是因为它是如此*简单*，你不相信它会工作。

同样，[Anthropic对该技术有很好的描述](https://www.anthropic.com/engineering/claude-think-tool)，详细介绍了他们的"think"工具，它基本上是一个草稿板：

> 使用"think"工具，我们让Claude能够包含一个额外的思考步骤——完整的专用空间——作为获得最终答案的一部分……这在执行长链工具调用或与用户进行长时间多步骤对话时特别有用。

我真的很欣赏Anthropic发布的研究和其他文章，但我不喜欢这个工具的名字。如果这个工具叫`scratchpad`，你会*立即*知道它的功能。这是模型写下不会污染其上下文、可供以后参考的笔记的地方。"think"这个名字与"[扩展思维](https://www.anthropic.com/news/visible-extended-thinking)"冲突，并且不必要地将模型拟人化……但我离题了。

有一个记录笔记和进度的空间*是有效的*。Anthropic显示将"think"工具与特定于领域的提示词配对（无论如何你都会在智能体中这样做）会产生显著收益，在专业智能体基准上提高多达54%。

Anthropic确定了三种上下文卸载模式有用的场景：

> 1. 工具输出分析。当Claude需要在行动前仔细处理先前工具调用的输出，并且可能需要回溯其方法时；
> 2. 策略繁重的环境。当Claude需要遵循详细指导原则并验证合规性时；以及
> 3. 顺序决策制定。当每个行动都建立在先前行动基础上且错误代价高昂时（通常出现在多步骤领域中）。

* * *

上下文管理通常是构建智能体最困难的部分。如Karpathy所说，编程LLM"[恰到好处地打包上下文窗口](https://x.com/karpathy/status/1937902205765607626)"，智能地部署工具、信息和常规上下文维护是智能体设计者的*工作*。

上述所有策略的关键洞察是*上下文不是免费的*。上下文中的每个标记都会影响模型的行为，无论好坏。现代LLM的大规模上下文窗口是一种强大的能力，但它们不是信息管理马虎的借口。

当你构建下一个智能体或优化现有智能体时，问问自己：这个上下文中的一切都在发挥其作用吗？如果没有，你现在有六种方法来修复它。

* * *

1. 见鬼，这整个策略列表有力地支持[你应该编程你的上下文](https://www.dbreunig.com/2025/06/10/let-the-model-write-the-prompt.html)。[↩](https://www.dbreunig.com/2025/06/26/how-to-fix-your-context.html#fnref:structure)