标题：长上下文如何失效

来源地址：https://www.dbreunig.com/2025/06/22/how-contexts-fail-and-how-to-fix-them.html

发布时间：2025-06-22T16:19:00-07:00

![Image 1](https://www.dbreunig.com/img/overload.jpg)

### 管理上下文是成功构建智能体的关键

随着前沿模型上下文窗口的不断扩大[1](https://www.dbreunig.com/2025/06/22/how-contexts-fail-and-how-to-fix-them.html#fn:longcontext)，许多模型现在支持多达100万个标记，我看到了许多令人兴奋的讨论，认为长上下文窗口将解锁我们梦想中的智能体。毕竟，有了足够大的窗口，你可以简单地将可能需要的一切——工具、文档、指令等等——都扔进提示词中，让模型来处理剩下的事情。

长上下文削弱了RAG的热情（当你可以将所有文档都放入提示词时，就不需要寻找最佳文档了！），推动了MCP的炒作（连接到每个工具，模型就能做任何工作！），并激发了对智能体的热情[2](https://www.dbreunig.com/2025/06/22/how-contexts-fail-and-how-to-fix-them.html#fn:googledocs)。

但实际上，更长的上下文并不会产生更好的响应。过载你的上下文可能会导致智能体和应用程序以令人意外的方式失效。上下文可能会被污染、分散注意力、造成混乱或产生冲突。这对于依赖上下文来收集信息、综合发现和协调行动的智能体来说尤其成问题。

让我们回顾一下上下文可能失控的方式，然后介绍缓解或完全避免上下文失效的方法。

上下文失效
---------

* [上下文污染：当幻觉进入上下文时](https://www.dbreunig.com/2025/06/22/how-contexts-fail-and-how-to-fix-them.html#context-poisoning)
* [上下文分散：当上下文压倒训练时](https://www.dbreunig.com/2025/06/22/how-contexts-fail-and-how-to-fix-them.html#context-distraction)
* [上下文混乱：当多余的上下文影响响应时](https://www.dbreunig.com/2025/06/22/how-contexts-fail-and-how-to-fix-them.html#context-confusion)
* [上下文冲突：当上下文的某些部分产生分歧时](https://www.dbreunig.com/2025/06/22/how-contexts-fail-and-how-to-fix-them.html#context-clash)

* * *

### 上下文污染

*上下文污染是指幻觉或其他错误进入上下文，并被反复引用。*

DeepMind团队在[Gemini 2.5技术报告](https://storage.googleapis.com/deepmind-media/gemini/gemini_v2_5_report.pdf)中指出了上下文污染问题，我们[上周对此进行了分析](https://www.dbreunig.com/2025/06/17/an-agentic-case-study-playing-pok%C3%A9mon-with-gemini.html)。在玩宝可梦时，Gemini智能体偶尔会在游戏过程中产生幻觉，从而污染其上下文：

> 这个问题的一个特别严重的形式可能是"上下文污染"——上下文的许多部分（目标、摘要）被关于游戏状态的错误信息"污染"，这往往需要很长时间才能撤销。因此，模型可能会固着于实现不可能或不相关的目标。

如果其上下文的"目标"部分被污染，智能体会制定荒谬的策略，并重复行为来追求无法实现的目标。

### 上下文分散

*上下文分散是指上下文变得太长，导致模型过度关注上下文，忽略了在训练期间学到的知识。*

在智能体工作流程中，随着模型收集更多信息并建立历史记录，上下文会增长——这种累积的上下文可能变得分散注意力而不是有帮助。玩宝可梦的Gemini智能体清楚地展示了这个问题：

> 虽然Gemini 2.5 Pro支持100万+标记上下文，但在智能体中有效利用它是一个新的研究前沿。在这种智能体设置中，观察到当上下文显著超过10万标记时，智能体表现出倾向于重复其庞大历史中的动作，而不是综合新颖的计划。这种现象虽然是轶事性的，但突出了检索用长上下文和多步骤生成推理用长上下文之间的重要区别。

智能体不是使用其训练来制定新策略，而是固着于重复其广泛上下文历史中的过去动作。

对于较小的模型，分散注意力的上限要低得多。[Databricks研究](https://www.databricks.com/blog/long-context-rag-performance-llms)发现，Llama 3.1 405b的模型正确性在大约32k标记左右开始下降，而较小模型的下降更早。

如果模型在其上下文窗口被填满之前就开始异常行为，那么超大上下文窗口的意义何在？简而言之：总结[3](https://www.dbreunig.com/2025/06/22/how-contexts-fail-and-how-to-fix-them.html#fn:summarization)和事实检索。如果你不是在做这两件事，要小心你选择的模型的分散注意力上限。

### 上下文混乱

*上下文混乱是指上下文中的多余内容被模型用来生成低质量响应。*

有那么一段时间，似乎*每个人*都要发布一个[MCP](https://www.dbreunig.com/2025/03/18/mcps-are-apis-for-llms.html)。一个强大的模型连接到你*所有的*服务和*东西*，完成你所有的平凡任务，这个梦想似乎触手可及。只需将所有工具描述扔进提示词然后开始。[Claude的系统提示词](https://www.dbreunig.com/2025/05/07/claude-s-system-prompt-chatbots-are-more-than-just-models.html)为我们指明了道路，因为它主要是工具定义或使用工具的指令。

但即使[整合和竞争不会减缓MCP](https://www.dbreunig.com/2025/06/16/drawbridges-go-up.html)，*上下文混乱*也会。事实证明，工具太多可能是个问题。

[伯克利函数调用排行榜](https://gorilla.cs.berkeley.edu/leaderboard.html)是一个工具使用基准，评估模型有效使用工具响应提示词的能力。现在已经是第三版，排行榜显示*每个*模型在提供超过一个工具时性能都会下降[4](https://www.dbreunig.com/2025/06/22/how-contexts-fail-and-how-to-fix-them.html#fn:live)。此外，伯克利团队"设计了没有提供的函数相关的场景……我们期望模型的输出是不调用函数。"然而，所有模型偶尔都会调用不相关的工具。

浏览函数调用排行榜，你可以看到随着模型变小，问题会变得更糟：

![Image 2](https://www.dbreunig.com/img/gemma_irrelevance.png)

上下文混乱的一个显著例子可以在[最近的一篇论文](https://arxiv.org/pdf/2411.15399?)中看到，该论文评估了小模型在[GeoEngine基准](https://arxiv.org/abs/2404.15500)上的性能，这是一个包含*46个不同工具*的测试。当团队给量化（压缩）的Llama 3.1 8b一个包含所有46个工具的查询时，它失败了，尽管上下文远在16k上下文窗口之内。但当他们只给模型19个工具时，它成功了。

问题是：如果你把某些东西放在上下文中，*模型就必须关注它*。它可能是不相关的信息或不必要的工具定义，但模型*会*考虑它。大型模型，特别是推理模型，在忽略或丢弃多余上下文方面越来越好，但我们持续看到无价值信息让智能体跌倒。更长的上下文让我们可以塞入更多信息，但这种能力伴随着缺点。

### 上下文冲突

*上下文冲突是指你在上下文中积累的新信息和工具与上下文中的其他信息发生冲突。*

这是*上下文混乱*的一个更有问题的版本：这里的坏上下文不是不相关的，它直接与提示词中的其他信息冲突。

微软和Salesforce团队在[最近的一篇论文](https://arxiv.org/pdf/2505.06120)中出色地记录了这一点。团队从多个基准中获取提示词，并将其信息"分片"到多个提示词中。这样想：有时，你可能会坐下来在ChatGPT或Claude中输入段落，在点击回车之前考虑每个必要的细节。其他时候，你可能从一个简单的提示词开始，然后当聊天机器人的答案不满意时添加进一步的细节。微软/Salesforce团队修改了基准提示词，使其看起来像这些多步骤交流：

![Image 3](https://www.dbreunig.com/img/sharded_prompt.png)

左侧提示词中的所有信息都包含在右侧的几条消息中，这些消息将在多轮聊天中播放。

分片提示词产生了显著更差的结果，平均下降39%。团队测试了一系列模型——OpenAI备受赞誉的o3的分数从98.1下降到64.1。

这是怎么回事？为什么当信息分阶段收集而不是一次性全部收集时，模型的表现会更差？

答案是*上下文混乱*：组装的上下文，包含整个聊天交换的内容，包含模型在拥有所有信息*之前*试图回答挑战的早期尝试。这些错误答案仍然存在于上下文中，并在模型生成最终答案时影响模型。团队写道：

> 我们发现LLM经常在早期轮次中做出假设并过早地尝试生成最终解决方案，然后过度依赖这些解决方案。简单来说，我们发现当LLM在对话中走错方向时，它们会迷失并不会恢复。

这对智能体构建者来说不是好兆头。智能体从文档、工具调用和其他被分配子问题的模型中组装上下文。所有这些上下文，从不同来源提取，都有可能与自身产生分歧。此外，当你连接到不是你创建的MCP工具时，它们的描述和指令与你提示词的其余部分发生冲突的可能性更大。

* * *

百万标记上下文窗口的到来感觉是变革性的。能够将智能体可能需要的一切扔进提示词的能力激发了对超智能助手的愿景，这些助手可以访问任何文档，连接到每个工具，并保持完美记忆。

但正如我们所看到的，更大的上下文创造了新的失效模式。上下文污染嵌入随时间复合的错误。上下文分散导致智能体严重依赖其上下文并重复过去的动作，而不是向前推进。上下文混乱导致不相关的工具或文档使用。上下文冲突创造内部矛盾，使推理脱轨。

这些失效对智能体的冲击最大，因为智能体恰好在上下文会膨胀的场景中运行：从多个来源收集信息，进行顺序工具调用，参与多轮推理，并积累广泛的历史记录。

幸运的是，有解决方案！在即将发布的文章中，我们将介绍缓解或避免这些问题的技术，从动态加载工具的方法到建立上下文隔离。

**阅读后续文章，"[如何修复你的上下文](https://www.dbreunig.com/2025/06/26/how-to-fix-your-context.html)"**

* * *

1. Gemini 2.5和GPT-4.1有100万标记上下文窗口，大到足以把[无尽玩笑](https://en.wikipedia.org/wiki/Infinite_Jest)扔进去，还有大量空间剩余。[↩](https://www.dbreunig.com/2025/06/22/how-contexts-fail-and-how-to-fix-them.html#fnref:longcontext)

2. Gemini文档中的"[长形式文本](https://ai.google.dev/gemini-api/docs/long-context#long-form-text)"部分很好地总结了这种乐观情绪。[↩](https://www.dbreunig.com/2025/06/22/how-contexts-fail-and-how-to-fix-them.html#fnref:googledocs)

3. 事实上，在上面引用的Databricks研究中，当给定长上下文时模型失效的一个常见方式是它们会返回提供的上下文的摘要，而忽略提示词中包含的任何指令。[↩](https://www.dbreunig.com/2025/06/22/how-contexts-fail-and-how-to-fix-them.html#fnref:summarization)

4. 如果你在排行榜上，请注意"Live (AST)"列。[这些指标使用企业向产品贡献的真实工具定义](https://gorilla.cs.berkeley.edu/blogs/12_bfcl_v2_live.html)，"避免了数据集污染和有偏基准的缺点。"[↩](https://www.dbreunig.com/2025/06/22/how-contexts-fail-and-how-to-fix-them.html#fnref:live)